{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Networks pt.1",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNDhIJnv94+aN83lP5UTzbR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WheelerMLClub/DataScience/blob/main/Neural_Networks_pt_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IP5mSEUg40Y"
      },
      "source": [
        "# All the imports you need are here. Feel free to import additional libraries, but try to stick with these\r\n",
        "import numpy as np"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSl3CayxfM3X"
      },
      "source": [
        "#Neural Networks Part 1:\r\n",
        "\r\n",
        "#Prerequisites:\r\n",
        "- 3b1b videos or viewed neural network lesson\r\n",
        "- About 1 hr\r\n",
        "\r\n",
        "Lets get Started!\r\n",
        "\r\n",
        "#The Layers:\r\n",
        "Recall the basic structure of a neural network from the 3b1b videos. We will be making a type of neural network called a Multi-Layer-Perceptron today which is composed of multiple layers of neurons. Each layer feeds forward its result to the next neurons until we reach the output layer.\r\n",
        "\r\n",
        "### Code Completion!\r\n",
        "Feeling a challange? Try deleing the code below and and making a layer called SimpleNeuronLayer. There are three functions to complete. In the \\_\\_init\\__ function intialize a matrix of size (input_shape,cells) to serve as our weights, then make a matrix of size (cells) to serve as the bais. Then in compute write some code to take the dot product of the inputs and the weights to compute the result of running these inputs through the layer. You may do this an alternate way, but keep in mind that we will have track how the cost function changes with the weights and the baises later. Finally make a get_output_shape function and make it return the numebr of cells. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24_YhWcHfL-P"
      },
      "source": [
        "class SimpleNeuronLayer():\r\n",
        "  def __init__(self,input_shape,cells):\r\n",
        "    self.input_shape = input_shape\r\n",
        "    self.cells = cells\r\n",
        "    self.weights = np.random.normal(size=(input_shape,cells))\r\n",
        "    self.baises = np.random.normal(size=(cells))\r\n",
        "  def compute(self,inputs):\r\n",
        "    return np.dot(inputs,self.weights) + self.baises\r\n",
        "  def get_output_shape(self):\r\n",
        "    return self.cells"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWk7C10_jCB7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b83ebde0-68b7-447a-d88f-d8d8011a4385"
      },
      "source": [
        "# Some Test Code that should work!\r\n",
        "A = SimpleNeuronLayer(10,5)\r\n",
        "A.compute(np.array([[1,2,3,4,5,6,7,8,9,10]]))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-5.03696726,  7.58463136, 14.4901609 ,  9.19760009,  7.47386731]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM-jodsClDrf"
      },
      "source": [
        "## The Activation Function\r\n",
        "\r\n",
        "Activation functions are essential for neural networks to learn relationships in nonlinear data. Lets write a simple one called ReLU or the Rectified Linear Unit. It just takes the inputs and if its negative sets ReLU sets the input to zero and returns it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJwmjTMcjJVM"
      },
      "source": [
        "class ReLU():\r\n",
        "  def __init__(self,input_shape):\r\n",
        "    self.input_shape = input_shape\r\n",
        "  def compute(self,inputs):\r\n",
        "    return inputs*(inputs>0)\r\n",
        "  def get_output_shape(self):\r\n",
        "    return self.input_shape"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRdwlJFemgkO"
      },
      "source": [
        "##Stacking the Layers into a Network\r\n",
        "\r\n",
        "Now Lets make a Network that will stack the layers and pass in the relevant values into the intialization functions to properly build the layers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDIRdy5cmfUA"
      },
      "source": [
        "class Network():\r\n",
        "  def __init__(self,input_shape):\r\n",
        "    self.shapes = [input_shape]\r\n",
        "    self.layers = []\r\n",
        "  def add(self,layer,*args):\r\n",
        "    self.layers.append(layer(self.shapes[-1],*args))\r\n",
        "    self.shapes.append(self.layers[-1].get_output_shape())\r\n",
        "  def compute(self,inputs):\r\n",
        "    last_output = inputs\r\n",
        "    for i in self.layers:\r\n",
        "      last_output = i.compute(last_output)\r\n",
        "    return last_output"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAch7FuHnclM"
      },
      "source": [
        "N = Network(5)\r\n",
        "N.add(SimpleNeuronLayer,10)\r\n",
        "N.add(ReLU)\r\n",
        "N.add(SimpleNeuronLayer,20)\r\n",
        "N.add(ReLU)\r\n",
        "N.add(SimpleNeuronLayer,5)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ0FBan4ndyr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8bf5f67-f1cd-4c66-ccf7-06a3b7525f81"
      },
      "source": [
        "N.compute(np.array([1,2,3,4,5]))"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-20.50330568, -38.24793988, -31.65717237,  33.96143781,\n",
              "        60.31301579])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAd0fDPVtRu3"
      },
      "source": [
        "Bam, and just like that we have made a neural network. Tune in next time to learn how to train this neural network to learn a certain set of data"
      ]
    }
  ]
}